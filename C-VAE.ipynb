{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf74683",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "<h2>CRVAE t-SNE for Unsupervised Feature Learning: Image Classification of Flower </h2>\n",
    "<p>\n",
    "This project explores the utilization of machine learning (ML) strategies for unsupervised feature learning and classification of flower images. Leveraging the capabilities of a Convolutional ResNet Variational AutoEncoder (CRVAE) combined with t-Distributed Stochastic Neighbour Embedding (t-SNE) clustering, we aim to derive meaningful and separable feature representations of flower datasets.</p>\n",
    "<p>The dataset contains 2,669 unlabelled images of flowers (and junk images) that fit into one 5 classes in the training set and the test set has 1,000 labelled images, 200 from each category.\n",
    "</p>\n",
    "<p>Example of train data:\n",
    "<table>\n",
    "    <td><img src=\"./pics_for_notebook/1.jpg\" alt=\"1\" width=\"150\" height=\"150\"></td>\n",
    "    <td><img src=\"./pics_for_notebook/2.jpg\" alt=\"2\" width=\"150\" height=\"150\"></td>\n",
    "    <td><img src=\"./pics_for_notebook/3.jpg\" alt=\"3\" width=\"150\" height=\"150\"></td>\n",
    "    <td><img src=\"./pics_for_notebook/4.jpg\" alt=\"4\" width=\"150\" height=\"150\"></td>\n",
    "</table>\n",
    "</p>\n",
    "\n",
    "<h2>AutoEncoders</h2>\n",
    "<p>\n",
    "Sources:\n",
    "https://paperswithcode.com/method/autoencoder ; https://arxiv.org/pdf/2003.05991.pdf\n",
    "</p><p>\n",
    "An autoencoder is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder). Their main purpose is learning in an unsupervised manner an “informative” representation of the data that can be used for various implications such as clustering.\n",
    "<img src=\"./pics_for_notebook/autoencoder.png\" alt=\"4\" width=\"300\" height=\"300\"></p>\n",
    "<h2>VAE</h2>\n",
    "<p>\n",
    "Sources:\n",
    "https://www.techtarget.com/searchenterpriseai/definition/variational-autoencoder-VAE\n",
    "</p><p>\n",
    "A variational autoencoder (VAE) is a generative AI algorithm that uses deep learning to generate new content, detect anomalies and remove noise. \n",
    "</p>\n",
    "<h2>ResNet</h2>\n",
    "<p>\n",
    "Sources:\n",
    "https://par.nsf.gov/servlets/purl/10225365 ; https://github.com/JayPatwardhan/ResNet-PyTorch/tree/master\n",
    "</p><p>\n",
    "Efficient modeling of high-dimensional data requires extracting only relevant dimensions through feature learning. The advantage of RAE and C-RAE is that it enables the user to add residual connections for increased network capacity without incurring the cost of degradation for unsupervised feature learning compared to standard AEs.\n",
    "</p>\n",
    "<p>The resnetblock: <img src=\"./pics_for_notebook/resnetblock.png\" alt=\"4\" width=\"250\" height=\"250\"></p>\n",
    "<h2>t-SNE</h2>\n",
    "<p>\n",
    "Sources:\n",
    "https://www.datacamp.com/tutorial/introduction-t-sne ; https://www.kaggle.com/code/rohitgr/autoencoders-tsne\n",
    "</p><p>\n",
    "t-SNE is an unsupervised non-linear dimensionality reduction technique for data exploration and visualizing high-dimensional data. Non-linear dimensionality reduction means that the algorithm allows us to separate data that cannot be separated by a straight line. The t-SNE algorithm finds the similarity measure between pairs of instances in higher and lower dimensional space. After that, it tries to optimize two similarity measures. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c87ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# for progress bar\n",
    "from tqdm import tqdm as tq\n",
    "\n",
    "#for clustring \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#for ploting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89849c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    \n",
    "def imshow_reconst(img):\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    \n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 4\n",
    "IMG_SIZE = 128\n",
    "H_DIM = 5\n",
    "PATH = r'.\\Wieghts'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize the images to IMG_SIZExIMG_SIZE\n",
    "    transforms.ToTensor(),  # Convert images to Tensor\n",
    "])\n",
    "\n",
    "unlabeled_data_path = r'C:\\Users\\shirl\\Documents\\imagegene_flowers\\DS_dataset'\n",
    "dataset = datasets.ImageFolder(root=unlabeled_data_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db000a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model\n",
    "class ResNetBlockEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, downsample=None):\n",
    "        super(ResNetBlockEncoder, self).__init__()\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        # --------- conv 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # --------- conv 2\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "            \n",
    "        layer1 = self.conv1(x)\n",
    "        layer1 = self.bn1(layer1)\n",
    "        layer1 = self.relu(layer1)\n",
    "\n",
    "        layer2 = self.conv2(layer1)\n",
    "        layer2 = self.bn2(layer2)\n",
    "        layer2 = self.relu(layer2)\n",
    "        layer2 = self.conv2(layer1)\n",
    "        layer2 = self.bn2(layer2)\n",
    "        layer2 = self.relu(layer2)\n",
    "\n",
    "        out_layer = torch.add(layer1, layer2)\n",
    "        out_layer = self.relu(out_layer)\n",
    "\n",
    "        return out_layer\n",
    "    \n",
    "class ResNetBlockDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, upsample=None):\n",
    "        super(ResNetBlockDecoder, self).__init__()\n",
    "        \n",
    "        self.upsample = upsample\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        # --------- t-conv 1\n",
    "        self.tconv1 = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # --------- t-conv 2\n",
    "        self.tconv2 = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.upsample = upsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.upsample is not None:\n",
    "            x = self.upsample(x)\n",
    "            \n",
    "        layer1 = self.tconv1(x)\n",
    "        layer1 = self.bn1(layer1)\n",
    "        layer1 = self.relu(layer1)\n",
    "\n",
    "        layer2 = self.tconv2(layer1)\n",
    "        layer2 = self.bn2(layer2)\n",
    "        layer2 = self.relu(layer2)\n",
    "        layer2 = self.tconv2(layer1)\n",
    "        layer2 = self.bn2(layer2)\n",
    "        layer2 = self.relu(layer2)\n",
    "\n",
    "        out_layer = torch.add(layer1, layer2)\n",
    "        out_layer = self.relu(out_layer)\n",
    "\n",
    "        return out_layer\n",
    "\n",
    "class CRVAE(nn.Module):\n",
    "    def __init__(self, img_size, stride, latent_dim):\n",
    "        super(CRVAE, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        linear_layer = int((self.img_size - 3 + 2)/stride) +1  #( (I-K+2P)/(S) + 1)\n",
    "        self.linear_layer = int(linear_layer / 4) # beacuse MaxPooling\n",
    "        \n",
    "        self.relu = nn.LeakyReLU()\n",
    "        \n",
    "        # encoder\n",
    "        self.resnet1 = ResNetBlockEncoder(in_channels=3, out_channels=16, stride=stride)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, return_indices=True)\n",
    "        \n",
    "        self.resnet2 = ResNetBlockEncoder(in_channels=16, out_channels=32, stride=stride)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, return_indices=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(self.linear_layer*self.linear_layer*32, int(self.linear_layer*self.linear_layer*32/16))\n",
    "        self.linear3 = nn.Linear(int(self.linear_layer*self.linear_layer*32/16), int(self.linear_layer*self.linear_layer*32/64))\n",
    "        self.fc_mu = nn.Linear(int(self.linear_layer*self.linear_layer*32/64), latent_dim)\n",
    "        self.fc_var = nn.Linear(int(self.linear_layer*self.linear_layer*32/64), latent_dim)\n",
    "        \n",
    "        # decoder\n",
    "        self.unlinear0 = nn.Linear(latent_dim, int(self.linear_layer*self.linear_layer*32/64))\n",
    "        self.unlinear1 = nn.Linear(int(self.linear_layer*self.linear_layer*32/64), int(self.linear_layer*self.linear_layer*32/16))\n",
    "        self.unlinear3 = nn.Linear(int(self.linear_layer*self.linear_layer*32/16) , self.linear_layer*self.linear_layer*32)\n",
    "        \n",
    "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2)\n",
    "        self.unresnet2 = ResNetBlockDecoder(in_channels=32, out_channels=16, stride=1)\n",
    "        \n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "        self.unresnet1 = ResNetBlockDecoder(in_channels=16, out_channels=3, stride=1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.batch_size = x.size(0)\n",
    "        x = self.resnet1(x)\n",
    "        x, self.indices1 = self.pool1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.resnet2(x)\n",
    "        x, self.indices2 = self.pool2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        \n",
    "        z = self.unlinear0(z)\n",
    "        z = self.relu(z)\n",
    "        \n",
    "        z = self.unlinear1(z)\n",
    "        z = self.relu(z)\n",
    "        \n",
    "        z = self.unlinear3(z)\n",
    "        z = self.relu(z)\n",
    "\n",
    "        z = z.view(self.batch_size, 32, self.linear_layer, self.linear_layer)\n",
    "\n",
    "        z = self.unpool2(z, self.indices2)\n",
    "        z = self.unresnet2(z)\n",
    "        z = self.relu(z)\n",
    "        \n",
    "        z = self.unpool1(z, self.indices1)    \n",
    "        z = self.unresnet1(z)\n",
    "        \n",
    "        return torch.sigmoid(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13851cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataloader, encoder):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for data, _ in dataloader:\n",
    "            mu, logvar = encoder(data.to(device))\n",
    "            features.append(mu.cpu())\n",
    "\n",
    "    # Concatenate all features into a single tensor\n",
    "    features = torch.cat(features, dim=0)\n",
    "    return features\n",
    "\n",
    "def showResults(model):\n",
    "    # Graph\n",
    "    test_data_path = r'.\\DS_dataset\\test'\n",
    "    test_dataset = datasets.ImageFolder(root=test_data_path, transform=transform)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_features = extract_features(test_dataloader, model.encode)\n",
    "    test_features = test_features.numpy()\n",
    "    true_labels = [label for _, label in test_dataset]\n",
    "    tsne = TSNE(n_components=2)\n",
    "    reduced_features = tsne.fit_transform(test_features)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot reduced features in 2D, colored by true class labels\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=true_labels, palette=\"bright\")\n",
    "    plt.title('Train Labels')\n",
    "    plt.show()\n",
    "\n",
    "    data, _ = next(iter(train_loader))\n",
    "    data_reconst, mu, sigma = model(data)\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        imshow(data[i])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    for i in range(4):\n",
    "        plt.subplot(2,2, i + 1)\n",
    "        imshow_reconst(data_reconst[i])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "def train(num_epochs, model, optimizer, scheduler):\n",
    "    for epoch in range(num_epochs):\n",
    "        # progress bar\n",
    "        print(\"epoch number:\", epoch+1)\n",
    "        loop = tq(enumerate(train_loader))\n",
    "        for i, (feature, label) in loop:\n",
    "            # Forward pass\n",
    "            feature = feature.to(device)\n",
    "            feature_reconst, mu, log_var = model(feature)\n",
    "\n",
    "            #clac loss\n",
    "            rec_loss = F.mse_loss(feature_reconst, feature)\n",
    "            kl_div = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim=0)\n",
    "            total_loss = rec_loss + kl_div.mean()\n",
    "            \n",
    "            # Backprop and optimize\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            \n",
    "            #optimizer            \n",
    "            optimizer.step()\n",
    "            loop.set_postfix(loss=total_loss.item())     \n",
    "            \n",
    "            \n",
    "        # Update the learning rate\n",
    "        scheduler.step()            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5554963",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.001\n",
    "\n",
    "conv_rvae = CRVAE(img_size=IMG_SIZE, stride=1, latent_dim=H_DIM).to(device)\n",
    "optimizer = optim.Adam(conv_rvae.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "train(NUM_EPOCHS, conv_rvae, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac761f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "showResults(conv_rvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d01fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seeing results with the weights achived after 20 epochs\n",
    "conv_rvae = CRVAE(img_size=IMG_SIZE, stride=1, latent_dim=H_DIM).to(device)\n",
    "conv_rvae.load_state_dict(torch.load(PATH))\n",
    "conv_rvae.eval()\n",
    "showResults(conv_rvae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
